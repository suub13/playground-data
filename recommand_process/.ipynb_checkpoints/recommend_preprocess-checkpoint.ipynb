{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8b7f259",
   "metadata": {},
   "source": [
    "# import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82f40f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "from konlpy.tag import Komoran\n",
    "\n",
    "import pandas as pd\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7c27f0",
   "metadata": {},
   "source": [
    "# skills 추출 후 중복 제거하고 skills_set list 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68813186",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_skills_set():\n",
    "    skills_list = []\n",
    "    skills_filter = []\n",
    "\n",
    "    with open(\"data/combined_crawling_data.csv\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f)\n",
    "\n",
    "        for row in reader:\n",
    "            # row 리스트의 10번째 값을 가져와 string 을 '' 안의 단어들로 나눠서 리스트로 만들기\n",
    "            skills = re.findall(r\"'(.*?)'\", row[10])\n",
    "            skills_upper = [word.upper() for word in skills]\n",
    "\n",
    "            # 나누어진 리스트를 추가\n",
    "            skills_list += skills_upper\n",
    "\n",
    "        skills_set = set(skills_list)\n",
    "\n",
    "    return skills_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c91333d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "659\n",
      "{'리팩토링', '소켓', 'SOC', '분석 능력', '자동차', 'SSL', '신호 처리', 'VR', 'REDUX', 'FLOW', '인공 지능', 'MICROSOFT SERVER', 'TCP', 'BLOCKCHAIN', '회로', '윈도우 프로그래밍', '3D 그래픽', '서버 아키텍처', 'CMS', 'PHPUNIT', 'GPS', 'IOS 디자인', 'CRM', '통계 모델링', '애자일 프로젝트 관리', 'PCB 레이아웃 설계', '공차 분석', '보안 감사', 'GCC', '임베디드 소프트웨어', 'RHEL', '프로세스 개선', 'VISUAL STUDIO CODE', '웹 디자인', '시스템 관리', '레이더', '강의', 'AIX', '의료 영상', '모바일 장치', 'SPSS', 'RTL 검증', 'GIMP', 'MICROCHIP PIC', 'SAP 구현', '네트워크 보안', 'SKETCH', '하드웨어 테스트', '마이크로프로세서', 'NGINX', 'SSIS', 'PYTHON', 'WINDOWS 원격 데스크톱', 'JPA', 'ADOBE ILLUSTRATOR', 'MIMO', 'ORACLE DATABASE', 'FPGA 프로토 타이핑', 'WINDOWS 서비스', '로봇', '네트워크 관리', 'SASS', '윈도우 모바일', 'XAMARIN', 'RESTFUL WEBSERVICES', 'PKI', 'SAP ERP', 'SWIFT', '기술 개발', 'AUTOCAD', 'PHP', 'IOS', '컴퓨터 공학', 'SHELL', 'WINDOWS EMBEDDED', '데이터 분석', '테스트 실행', 'REDIS', 'SAP MM', 'COCOA TOUCH', 'ASIC', 'GOOGLE APPS', 'PACS', '라이브러리 관리', '위성 통신', 'METAL', 'C / C++', 'AR', 'PYTORCH', 'CONFLUENCE', 'DEBIAN', 'AJAX', 'CCNA', 'ENCASE', 'BGP', '자연 언어 처리', '컴퓨터 비전', 'RS232', '시스템 통합', '인프라', 'HADOOP', 'CODEIGNITER', 'GITHUB', 'RASPBERRY PI', 'JQUERY UI', 'IIS', 'WEBGL', 'TYPEFORM', 'EXPRESSJS', 'ADC', 'SQLITE', 'POP 디스플레이', '모바일 게임 개발', '클라이언트 개발', '자동차 전자', 'RF', 'MAC', 'INTELLIJ IDEA', 'MDX', 'SIP', 'DICOM', 'AMAZON REDSHIFT', '비즈니스 분석', 'NEST.JS', '수학', 'GOOGLE TAG MANAGER', 'VUEJS', '프로토 타이핑', 'SERVLETS', 'API', 'EMC 규정 준수', '학술 연구', 'GLSL', 'UI 디자인', '프로젝트 납품', 'IBATIS', '알고리즘 설계', 'TDD', 'GOOGLE WEBMASTER TOOLS', 'SAP BI', 'PMP', 'SOLARIS ', 'RUBY', 'CISA', 'CTI', '앱 서버', 'TEAMCITY', 'DATASTAGE', 'C++', 'MS 오피스', 'ALTIUM', 'AGILE', '안드로이드 SDK', '일본어', 'VOIP', 'WPF', '교육 기술', '운영체제', '회로 설계', 'WORDPRESS DESIGN', '보안 정책', 'AUDIT COMMAND LANGUAGE (ACL)', 'IPS', 'GCP', 'ECLIPSE', 'OPENSTACK', 'DATA ANALYSIS', '정보 보안', 'NLP', 'RFP', 'MES', 'SSAS', '딥 러닝', 'UDP', 'TRELLO', 'ASP', '시스템 구축', 'SVN', 'CISCO', 'WEB SOCKET', 'FIREBASE', 'BASH', 'HTML5', 'CSS 자바 스크립트', '데이터 통합', 'IT 관리', 'SOLIDWORKS', 'ES6', 'APACHE SPARK', 'IT 운영', '분석능력', 'WBS', 'DOM', 'SAP 포털', '인버터', 'DEBUGGING', 'CAM', '카산드라', 'CSS3', 'IP', '하드웨어', 'JQUERY', '클라우드 보안', 'DART FOR PUBLISHERS', '모뎀', '재고 관리', 'NEXACRO', '관리', 'NUMPY', 'TABLEAU', 'RPG', 'SAP', 'SSRS', 'JUNIT', 'MXNET', 'RX', 'EDA', 'OPENCL', 'C#', 'GO', 'CITRIX', 'ELIXIR', 'ISR', 'NFC', 'SONET', 'FLASK', 'TESTNG', 'XP', '암호화', 'EMBEDDED LINUX', 'JSON', 'ASANA', 'RAID', 'DHCP', '웹 개발', '샘플 관리', 'SPI', '보안 금융', '클라우드 컴퓨팅', 'UX 디자인', 'JUNIPER', 'TCL', '3D', 'SLACK', 'UNIX', 'POWERBUILDER', '데이터 구조', '전원 엔지니어링', 'XAML', 'GRAPHQL', 'KERNEL PROGRAMMING', 'MFC', 'UBUNTU', 'XCODE', '복제', 'OPENCV', 'APPLE SOFTWARE', 'JSONAPI', 'UX 기획', 'NOSQL', '프로젝트 관리', 'OSS', 'SHADER', 'BI', '고객 관계', 'PCB 디자인', 'DJANGO', 'HYPERLEDGER', '품질 관리', 'SPARK', 'MYSQL', 'GOOGLE CLOUD PLATFORM', '서비스 관리', '모터 제어', 'APACHE KAFKA', 'HEROKU', 'DLP', 'ELASTICSEARCH', 'ABAP', 'WORDPRESS', '네트워크 엔지니어링', 'SVG', '분석 문제 해결', '모바일 기술', '분석 개발', 'RHINO 3D', 'ORACLE RAC', '머신 비전', 'POWER BI', '성능 측정', 'HEALTH INFORMATION TECHNOLOGY', 'TELERIK', '서비스 프로세스', 'SAAS', 'HTML', '인터페이스', 'STL', '검색 엔진', 'CAKEPHP', 'FPGA', 'SEO', 'SQL 서버', '품질 보증', 'SAS', '금융', '제어 시스템 설계', 'VDI', 'PAAS', '백신', 'B2C', 'LIN', '기술 문서', 'SMTP', 'DIAGNOSTIC', '샘플 준비', 'BITBUCKET', 'STP', 'CAN', 'SQL', 'KUBERNETES', 'KYC', 'C', '기술 교육', '알고리즘 개발', 'REACT NATIVE', '고객 지원', 'AZURE', 'CRM 소프트웨어', '서버', 'PLC', 'QT', '의료 산업', '전자 공학', 'POS', 'JSTL', '애자일 방법론', 'FTP', 'R', 'GOLANG', 'ENTITY FRAMEWORK', 'ORACLE', 'VUE.JS', '회로 분석', '서비스 디자인', '시스템 구현', 'SCRUM', 'ERP 소프트웨어', 'OTN', 'SAP BW', 'SKETCHUP', 'ADOBE XD', '요구 사항 분석', '네트워크 운영', 'DEVOPS', 'DIRECTX', 'MMO', 'CRM 데이터베이스', '모바일 게임', 'APPLICANT TRACKING SYSTEM(ATS)', 'ETL', 'GPU', 'HMI', 'SAP SD', 'OBJECTIVEC', 'GRUNTJS', 'JENKINS', '프레젠테이션', 'POWERSHELL', 'VBA', '기계 학습 (MACHINE LEARNING)', 'CLUSTERING', 'PADS', 'DB2', 'DOCKER', 'ROS', 'VMWARE INFRASTRUCTURE', 'UNITY3D', 'SPRING BOOT', 'SSH', '스토리지 영역 네트워크', 'MVC', 'B2B', 'SDLC', 'GOOGLE ANALYTICS', 'WINDOWS 서버', 'API 개발', 'VHDL', '클라우드 스토리지', 'MOBILE APP DESIGN', 'REACT.JS', 'HCI', 'VERILOG', 'OLAP', 'DSL', 'GIS 시스템', '모델링', '프로토콜 개발', '스토리지 솔루션', '설계', 'LINUX', 'HTA', 'ARDUINO', '보안 관리', 'SYBASE', '프론트엔드 개발자', '.NET CORE', 'SCALA', 'XML', 'ORM', 'GIS', '펌웨어', 'ZEMAX', 'AXURE', 'ORCAD', 'IVR', 'EMBEDDED C', '취약점 스캐닝', 'JIRA', 'CISSP', '전력전자학', '보안', 'XHTML', '프로그램 관리', 'ETL 도구', '차량', 'SCIKIT-LEARN', 'CRM 통합', '영상', 'ASP .NET', '리눅스 커널', 'EC2', '최적화', 'BOOTSTRAP', '생산 계획', 'UNREAL ENGINE', 'OFDM', 'REDMINE', 'FASTAPI', '유지보수', 'NEO4J', 'USER EXPERIENCE', 'REVIT', 'MVVM', 'SVELTE', '전자정부프레임워크', 'WPF 개발', 'XILINX', 'AAC', 'SNMP', 'RESTFUL API', '로봇 프로그래밍', '학술 출판', '의료 장비', '안드로이드 개발', '클라우드 응용 프로그램', 'FMEA', 'CCNP', 'MATLAB', 'JSP', '솔루션 아키텍처', 'AKKA', 'SCIPY', 'MULTI TASKING', 'SPINE', '웹사이트', '카메라', 'STORAGE', '네트워크 프로그래밍', 'QLIKVIEW', 'ADOBE', 'JAVASCRIPT', '아날로그 회로 설계', 'GIT', 'POSTGRESQL', 'FLUTTER', 'RUST', 'OPENGL', 'JSP 개발', '상품 기획', '구매 관리', '백엔드 개발', '보안 교육', 'ANDROID', 'SHELL SCRIPTING', 'RTP', 'GUI', '브랜딩', '통신', '네트워크 개발', 'NAS', 'VISUAL BASIC', '데이터 마트', 'UX', 'CENTOS', 'VM웨어', '검증', 'WINFORM', 'NODE.JS', 'RABBITMQ', '공공 부문', '서버 관리', 'JAVA', 'QA 엔지니어링', '네트워크 설계', '프로젝트 실행', '데이터 수집', 'CCIE', '자산 관리', 'ERP 구현', 'VB .NET', 'GIS 응용 프로그램', '방화벽', 'OPENMP', '가상화', 'I2C', 'LINQ', '네트워크 인프라', '도메인 관리', 'WINDOWS 8', '모델링 및 시뮬레이션', '분석 서비스', 'COMMUNICATION', '기술 관리', 'WEBRTC', 'VISUAL STUDIO', '.NET', 'APACHE', 'GITLAB', '정보관리', 'EAI', 'CHILDREN', '리눅스 시스템 관리', 'KOTLIN', 'SCM', 'OFFICE 365', '고객 중심', '개발', 'DNS', 'STRUTS', 'FRAMER', 'ADOBE PHOTOSHOP', 'LIDAR', 'ISO', 'SPA', 'EXCEL', 'OAUTH', 'DELPHI', '3D 모델링', 'HTTP', '반도체', 'TENSORFLOW', 'SAP FI', 'RTL 코딩', 'IOT', 'SPLUNK', 'XEN', 'ANDROID STUDIO', 'DWDM', 'SD', '구글 API', 'APACHE 2', 'MSSQL', '연구 및 개발', 'HP', '네트워크 통신', 'USER STORIES', 'FLASH', '데이터 시각화', '네비게이션 시스템', 'AWS', 'EMR', 'PADS LAYOUT', 'GOOGLE MAPS', 'SAP FICO', '교육 관리', 'SOLR', 'HILT', 'LIS', 'ORACLE SQL', '데이터베이스 관리', '예측 모델링', 'COROUTINE', 'CAD', '영어 실력', 'TOMCAT', '하드웨어 설계', 'CSS', 'IOS 개발', 'ITIL', '제품 개발', 'DSP', 'SAP HR', 'RESTFUL ARCHITECTURE', 'PALANTIR', '리눅스 서버', 'NODEJS', 'RDBMS', '그래픽 디자인', '3G', 'STATA', '데이터 입력', 'EMC STORAGE', 'SPRING FRAMEWORK', 'DBT', '의료 기기', 'MONGODB', 'RTOS', 'ANSI C', '보안 운영', 'GRADLE', 'RTL 설계', 'VPN', 'ANGULAR', 'PERFORCE', '임베디드 시스템', 'RETROFIT2', 'PRODUCT MANAGEMENT ', 'SMS', 'OOP', 'REST', '컴파일러', 'AMAZON WEB SERVICE', 'ETHEREUM', '데이터베이스', 'MAVEN', '병렬 프로그래밍', 'ZEPLIN', 'ML', '소프트웨어 개발', 'PROCESSING', '소프트웨어 검증', '데이터베이스 설계', '운영 관리', 'ANGULARJS', 'LARAVEL', 'NEXT.JS', 'AMPLITUDE', 'ACTIVE DIRECTORY', 'RUBY ON RAILS', '솔루션 개발', 'NOTION', 'LTE', 'ASTERISK', '제안서 작성', 'GCPS', 'WEBSPHERE MQ', 'TYPESCRIPT', 'CUDA', '무선 통신', 'PERL', 'QC', 'PMO', 'FIGMA', '빅 데이터', 'MIXPANEL', 'LESS', 'SIMULINK', 'AUTOSAR', 'REACT'}\n"
     ]
    }
   ],
   "source": [
    "skills_set = get_skills_set()\n",
    "print(len(skills_set))\n",
    "print(skills_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405498e2",
   "metadata": {},
   "source": [
    "# 리스트를 딕셔너리로 변환 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d1fd5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_dict(lst):\n",
    "    toDict = dict()\n",
    "\n",
    "    for item in lst:\n",
    "        toDict[item.upper()] = 0\n",
    "\n",
    "    return toDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63a05b6",
   "metadata": {},
   "source": [
    "# 딕셔너리를 CSV 파일로 저장하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aef48a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_csv_file(dict, title):\n",
    "    with open(title, \"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        writer.writerow([\"Key\", \"Kalue\"])\n",
    "        \n",
    "        for k, v in dict.items():\n",
    "            writer.writerow([k, v])\n",
    "            \n",
    "    print(\"Dictionary keys and values saved to CSV file successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc75ca5a",
   "metadata": {},
   "source": [
    "# 각 채용 링크의 qualification 속 skills_set 정리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2adc9093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qualification_skill_list():\n",
    "    com_set = get_skills_set()\n",
    "    com_list = list(com_set)\n",
    "    com_dict = list_to_dict(com_list)\n",
    "    \n",
    "    skill_dict = com_dict.copy()\n",
    "    freq_dict = dict()\n",
    "    \n",
    "    with open(\"data/combined_crawling_data.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        data = list(reader)\n",
    "        count = 1\n",
    "        \n",
    "        for row in tqdm(data[1:]):\n",
    "            id = row[0]\n",
    "            \n",
    "            # qualifications = 7, preferred = 8, skills = 10\n",
    "            quali_data = row[7].upper()\n",
    "            \n",
    "            # 확인해야하는 qualification data\n",
    "            phrases = com_list\n",
    "            \n",
    "            # remove symbols\n",
    "            sentence = re.sub(r'[^\\w\\s]', '', quali_data)\n",
    "            \n",
    "            # re.escape 로 특수문자를 벗어나고, | 로 단어 연결\n",
    "            pattern = r'\\b(' + '|'.join(map(re.escape, phrases)) + r')\\b'\n",
    "            \n",
    "            # 한글 정렬 후 나누기\n",
    "            komoran = Komoran()\n",
    "            sentence = komoran.morphs(sentence)\n",
    "            \n",
    "            # 한글 정렬 후 나누고 다시 문장으로 합쳐주기\n",
    "            sentence = \" \".join(sentence)\n",
    "            \n",
    "            # phrases 에 있는 단어를 가져와서 sentence 속 빈도 확인하기\n",
    "            matches = re.findall(pattern, sentence)\n",
    "            \n",
    "            if count % 500 == 0:\n",
    "                print(matches)\n",
    "            count += 1\n",
    "            \n",
    "            freq_dict[id] = list(set(matches))\n",
    "            \n",
    "    save_csv_file(freq_dict, \"data/freq_qual_data.csv\")\n",
    "    return freq_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e53551e",
   "metadata": {},
   "source": [
    "# 각 채용 링크의 preferred 속 skills_set 정리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b2e8987",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preferred_skill_list():\n",
    "    com_set = get_skills_set()\n",
    "    com_list = list(com_set)\n",
    "    com_dict = list_to_dict(com_list)\n",
    "    \n",
    "    skill_dict = com_dict.copy()\n",
    "    freq_dict = dict()\n",
    "    \n",
    "    with open(\"data/combined_crawling_data.csv\", 'r', encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        data = list(reader)\n",
    "        count = 1\n",
    "        \n",
    "        for row in tqdm(data[1:]):\n",
    "            id = row[0]\n",
    "            \n",
    "            # qualifications = 7, preferred = 8, skills = 10\n",
    "            data = row[8].upper()\n",
    "\n",
    "            # 확인해야 하는 데이터 (list)\n",
    "            phrases = com_list \n",
    "\n",
    "            # remove symbols\n",
    "            sentence = re.sub(r'[^\\w\\s]', '', data)\n",
    "\n",
    "            # re.escape으로 특수문자 벗어나고, |로 단어 연결\n",
    "            pattern = r'\\b(' + '|'.join(map(re.escape, phrases)) + r')\\b'\n",
    "\n",
    "            # 한글 정렬후 나누기\n",
    "            komoran = Komoran()\n",
    "            sentence = komoran.morphs(sentence)\n",
    "\n",
    "            # 한글 정렬 후 나누고 다시 문장으로 합쳐주기\n",
    "            sentence = \" \".join(sentence)\n",
    "            \n",
    "            # phrases에 있는 단어 sentence속 빈도 확인하기\n",
    "            matches = re.findall(pattern, sentence)\n",
    "            \n",
    "\n",
    "            if count % 500 == 0:\n",
    "                print(matches)\n",
    "            count += 1\n",
    "\n",
    "            # dictionary에 하나씩 넣어주기 - set으로 중복 제거\n",
    "            freq_dict[id] = list(set(matches))\n",
    "\n",
    "    save_csv_file(freq_dict, \"data/freq_pref_data.csv\")\n",
    "    return freq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ada8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▉                                                                                                        | 48/5451 [01:02<1:54:52,  1.28s/it]"
     ]
    }
   ],
   "source": [
    "get_preferred_skill_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b26fe4",
   "metadata": {},
   "source": [
    "# 각 채용 링크의 skills 속 skills_set 정리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98184832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_skills_skill_list():\n",
    "    freq_dict = dict()\n",
    "\n",
    "    with open(\"data/combined_crawling_data.csv\", 'r', encoding=\"utf-8\") as f:\n",
    "        skill_reader = csv.reader(f)\n",
    "        skill = list(skill_reader)\n",
    "\n",
    "        for row in skill[1:]:\n",
    "            id = row[0]\n",
    "            \n",
    "            data = row[10].upper()\n",
    "            \n",
    "            # id = 0,  skills = 10\n",
    "            freq_dict[id] = data\n",
    "\n",
    "    print(len(freq_dict))\n",
    "\n",
    "    save_csv_file(freq_dict, \"data/freq_skill_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c0c4ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5450\n",
      "Dictionary keys and values saved to CSV file successfully.\n"
     ]
    }
   ],
   "source": [
    "get_skills_skill_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb22454",
   "metadata": {},
   "source": [
    "# 각 리스트에 데이터를 넣어서 리스트 = 컬럼으로 한 csv 파일 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ea814aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xc0 in position 70: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/freq_qual_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m,encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m qual:\n\u001b[1;32m     11\u001b[0m     qual_reader \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mreader(qual)\n\u001b[0;32m---> 12\u001b[0m     qual_rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mqual_reader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     quals \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/freq_pref_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m,encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pref:\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.9/3.9.16/Frameworks/Python.framework/Versions/3.9/lib/python3.9/codecs.py:322\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;66;03m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[1;32m    321\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m--> 322\u001b[0m     (result, consumed) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;66;03m# keep undecoded input until the next call\u001b[39;00m\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m=\u001b[39m data[consumed:]\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xc0 in position 70: invalid start byte"
     ]
    }
   ],
   "source": [
    "total_dict = dict()\n",
    "\n",
    "ids = []\n",
    "\n",
    "with open(\"data/freq_skill_data.csv\",\"r\",encoding=\"cp949\") as skill:\n",
    "    skill_reader = csv.reader(skill)\n",
    "    skill_rows = list(skill_reader)\n",
    "    skills = []\n",
    "\n",
    "with open(\"data/freq_qual_data.csv\",\"r\",encoding=\"cp949\") as qual:\n",
    "    qual_reader = csv.reader(qual)\n",
    "    qual_rows = list(qual_reader)\n",
    "    quals = []\n",
    "\n",
    "with open(\"data/freq_pref_data.csv\",\"r\",encoding=\"utf-8\") as pref:\n",
    "    pref_reader = csv.reader(pref)\n",
    "    pref_rows = list(pref_reader)\n",
    "    prefs = []\n",
    "\n",
    "    print(len(skill_rows), len(qual_rows), len(pref_rows))\n",
    "    count = 0\n",
    "    for row in range(1, len(skill_rows)):\n",
    "        if (skill_rows[row][0] == qual_rows[row][0] == pref_rows[row][0]):\n",
    "            id = skill_rows[row][0]\n",
    "            skill = skill_rows[row][1]\n",
    "            qual = qual_rows[row][1]\n",
    "            pref = pref_rows[row][1]\n",
    "\n",
    "            # 각 리스트에도 넣어준다.\n",
    "            ids.append(id)\n",
    "            skills.append(skill)\n",
    "            quals.append(qual)\n",
    "            prefs.append(pref)\n",
    "\n",
    "        if row % 500 == 0:\n",
    "            print(skill_rows[row][0], qual_rows[row][0], pref_rows[row][0])\n",
    "            print(skill_rows[row][1], qual_rows[row][1], pref_rows[row][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10205d67",
   "metadata": {},
   "source": [
    "# quali, pref, skills 데이터 하나로 합치기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "83b4fc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataFrame\n",
    "df = pd.DataFrame({'id': ids, 'qualification': quals, 'preferred': prefs, 'skills': skills})\n",
    "\n",
    "# Save the DataFrame as a CSV file with column names\n",
    "df.to_csv('data/qual_pref_skill_combined.csv', index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1b0927",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
